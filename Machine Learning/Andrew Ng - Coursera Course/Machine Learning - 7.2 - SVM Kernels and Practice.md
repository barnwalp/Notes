## Kernels I

![000152](images/2020-10-18-000152.jpg)

$||x-l^{(1)}||$ is the Euclidian distance between x and $l^1$ 

![000153](images/2020-10-18-000153.jpg)

**Plotting values of $f^1 on$ $l^1$**

![000154](images/2020-10-18-000154.jpg)

let's have an example x near $l^1$, in this case $f^1$ will be equal to 1 and $f^2$ and $f^3$ will be equal to 0 as x is far from $l^2$ and $l^3$. In this way whenever feature x is near $l^i$, it will predict y = 1, and so a non-linear decision boundary will be formed around the landmarks.

![000155](images/2020-10-18-000155.jpg)

## Kernels II

![000156](images/2020-10-18-000156.jpg)

![000157](images/2020-10-18-000157.jpg)

Advanced optimization techniques that are available for SVM with kernels are not available for other algorithm such as logistics regression. Though it can be implemented, it will not be computationally efficient. 

![000158](images/2020-10-18-000158.jpg)

![000159](images/2020-10-18-000159.jpg)

## Using an SVM

![000160](images/2020-10-18-000160.jpg)

![000161](images/2020-10-18-000161.jpg)

![000162](images/2020-10-18-000162.jpg)

![000163](images/2020-10-18-000163.jpg)

- For SVM, you don't need to worry about local minima, it generate convex cost function.

![000164](images/2020-10-18-000164.jpg)

